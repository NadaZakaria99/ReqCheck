{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10616999,"sourceType":"datasetVersion","datasetId":6573334},{"sourceId":10621222,"sourceType":"datasetVersion","datasetId":6576334}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installation","metadata":{}},{"cell_type":"code","source":"!pip install numpy\n!pip install scipy\n!pip install chromadb -q\n!pip install xformers -q\n!pip install langchain -q\n!pip install flash_attn -q\n!pip install langchain-community -q\n!pip install sentence-transformers -q\n!pip install langchain-huggingface -q\n!pip install chromadb\n!pip install langchain_nvidia_ai_endpoints\n!pip install langgraph\n!pip install gradio\n!pip install PyPDF2 -q\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:06:22.432894Z","iopub.execute_input":"2025-01-30T20:06:22.433196Z","iopub.status.idle":"2025-01-30T20:07:25.814857Z","shell.execute_reply.started":"2025-01-30T20:06:22.433173Z","shell.execute_reply":"2025-01-30T20:07:25.813638Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\nRequirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.3,>=1.22.4->scipy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.3,>=1.22.4->scipy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.3,>=1.22.4->scipy) (2024.2.0)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from langchain_huggingface import HuggingFaceEmbeddings\nfrom tqdm.autonotebook import tqdm, trange\nfrom langchain_core.documents import Document\nimport chromadb\nfrom langchain_community.vectorstores import Chroma\nfrom langchain import hub\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom pprint import pprint\nfrom langgraph.graph import END, StateGraph, START\nfrom typing import List\nfrom langchain_core.documents import Document\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom typing_extensions import TypedDict\nimport pandas as pd\nimport torch\nimport os\nimport io\nfrom bs4 import BeautifulSoup","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:07:25.816390Z","iopub.execute_input":"2025-01-30T20:07:25.816718Z","iopub.status.idle":"2025-01-30T20:07:29.931207Z","shell.execute_reply.started":"2025-01-30T20:07:25.816685Z","shell.execute_reply":"2025-01-30T20:07:29.930531Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-16-c0feb6342847>:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm, trange\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"os.environ[\"TAVILY_API_KEY\"] = \"YOUR TAVILY_API_KEY\"\nos.environ[\"NVIDIA_API_KEY\"] =\"YOUR NVIDIA_API_KEY\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"RAG project\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:16:32.406179Z","iopub.execute_input":"2025-01-30T20:16:32.406580Z","iopub.status.idle":"2025-01-30T20:16:32.411369Z","shell.execute_reply.started":"2025-01-30T20:16:32.406551Z","shell.execute_reply":"2025-01-30T20:16:32.410143Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# Eembedding_model","metadata":{}},{"cell_type":"code","source":"model_name = \"dunzhang/stella_en_1.5B_v5\"\n\nmodel_kwargs = {\n                'trust_remote_code': 'True'\n                }\nembedding_model = HuggingFaceEmbeddings(model_name=model_name, show_progress=True, model_kwargs=model_kwargs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:07:29.933119Z","iopub.execute_input":"2025-01-30T20:07:29.933721Z","iopub.status.idle":"2025-01-30T20:10:33.491383Z","shell.execute_reply.started":"2025-01-30T20:07:29.933691Z","shell.execute_reply":"2025-01-30T20:10:33.490596Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d295580a9216459aa7e427e4b8310753"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/397 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb8e75de84bd453e86bf424d822ea3d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/169k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"519ac699b9a348a0b6f8bc26a6745f49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfda9316a7dd4126a21827abc7097d11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31a26c80cfc745e7ba31eaf02f35c1ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_qwen.py:   0%|          | 0.00/65.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ca362686d374dc59d180a68398f286d"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/dunzhang/stella_en_1.5B_v5:\n- modeling_qwen.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaf2c54bf6fb4c789b584fb5c2da08c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.31k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46168b40e39846719fad0a1ed7ed8a17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_qwen.py:   0%|          | 0.00/10.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78d7bb5b4d4543658569b8877577e983"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/dunzhang/stella_en_1.5B_v5:\n- tokenization_qwen.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15ba1773d73b46c3810eeb6dfb5ba8bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d74e48bb38d14e59a32f589186b60fb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b4d0fa8458a47dfaeb9bef1fbd74acb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/80.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e00858b3c9714756b64da451b0e48822"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/370 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbfa9aa3925a409db8d1a5c0874711a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6086171d3284d26b91bcdb6c831606b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"2_Dense_1024/config.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"368e5713db0c4447b292daef61fc08ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"208c3a22ca444c328269f38e572ee96c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/6.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3e87897bc29496dac08b633ac5d1b90"}},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"# Load your tool documentation.","metadata":{}},{"cell_type":"code","source":"dataiku_folder = \"/kaggle/working/flowise_docs\"\n\nd = {\"chunk\": [], \"url\": []}\n\nfor path in os.listdir(dataiku_folder):\n    url = \"https://\" + path.replace(\"=\", \"/\")\n    file_path = os.path.join(dataiku_folder, path)\n    \n    with open(file_path, 'rb') as stream:\n        content = stream.read().decode(\"utf-8\")\n        \n        # Parse the HTML content\n        soup = BeautifulSoup(content, \"html.parser\")\n        \n        # Extract the title\n        title = soup.find(\"title\")\n        title_text = title.string.replace(\" | Dataiku\", \"\") if title else \"No Title\"\n        \n        # Extract the main content (you might need to adjust this selector)\n        main_content = soup.find(\"main\")\n        if main_content:\n            text_content = main_content.get_text(strip=True)\n        else:\n            text_content = soup.get_text(strip=True)\n        \n        # Combine title and content\n        full_content = f\"{title_text}\\n\\n{text_content}\"\n        \n        # Add to our dictionary\n        d[\"chunk\"].append(full_content)\n        d[\"url\"].append(url)\n\n# Create a DataFrame from the dictionary\ndf = pd.DataFrame(d)\n\n# Display the first few rows of the DataFrame\nprint(df.head())\n\n# Optionally, save to a CSV file\n# df.to_csv(\"dataiku_docs_chunks.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:01:35.811692Z","iopub.execute_input":"2025-01-30T20:01:35.812018Z","iopub.status.idle":"2025-01-30T20:03:46.811283Z","shell.execute_reply.started":"2025-01-30T20:01:35.811997Z","shell.execute_reply":"2025-01-30T20:03:46.810341Z"}},"outputs":[{"name":"stdout","text":"                                               chunk  \\\n0  \\n   Read File | FlowiseAI\\n  \\n\\nIntegrations...   \n1  \\n   Introduction | FlowiseAI\\n  \\n\\nIntroduct...   \n2  \\n   Embed | FlowiseAI\\n  \\n\\nUsing FlowiseEmb...   \n3  \\n   VectorStore To Document | FlowiseAI\\n  \\n...   \n4  \\n   Introduction | FlowiseAI\\n  \\n\\nIntroduct...   \n\n                                                 url  \n0  https://integrations_langchain_tools_read-file...  \n1                      https://#retriever-nodes.html  \n2     https://using-flowise_embed?fallback/true.html  \n3  https://integrations_langchain_document-loader...  \n4  https://#database.tf-would-define-the-configur...  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"df[\"chunk_id\"] = range(len(df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:03:46.812464Z","iopub.execute_input":"2025-01-30T20:03:46.812890Z","iopub.status.idle":"2025-01-30T20:03:46.817466Z","shell.execute_reply.started":"2025-01-30T20:03:46.812848Z","shell.execute_reply":"2025-01-30T20:03:46.816644Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"list_of_documents = []\n\n# Converts the dataframe into list of dictionaries each dictionary has 'chunk' as key and its 'url' as values i.e. records\nrecords = df[['chunk', 'url']].to_dict(orient='records')\n\nfor record in records:\n    list_of_documents.append(\n        Document(\n            page_content = record['chunk'],\n            metadata={\"source_url\": record['url']}\n        )\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:03:57.410878Z","iopub.execute_input":"2025-01-30T20:03:57.411151Z","iopub.status.idle":"2025-01-30T20:03:58.632955Z","shell.execute_reply.started":"2025-01-30T20:03:57.411129Z","shell.execute_reply":"2025-01-30T20:03:58.632254Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(list_of_documents[:5][0].page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:04:24.767836Z","iopub.execute_input":"2025-01-30T20:04:24.768405Z","iopub.status.idle":"2025-01-30T20:04:24.773185Z","shell.execute_reply.started":"2025-01-30T20:04:24.768375Z","shell.execute_reply":"2025-01-30T20:04:24.772295Z"}},"outputs":[{"name":"stdout","text":"\n   Read File | FlowiseAI\n  \n\nIntegrationsLangChainToolsRead FileRead file from disk.PreviousCode Interpreter by E2BNextRequest GetLast updated6 days ago\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"df[['chunk', 'url']].to_dict(orient='records')[:2]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:04:46.665461Z","iopub.execute_input":"2025-01-30T20:04:46.665790Z","iopub.status.idle":"2025-01-30T20:04:46.678494Z","shell.execute_reply.started":"2025-01-30T20:04:46.665763Z","shell.execute_reply":"2025-01-30T20:04:46.677711Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[{'chunk': '\\n   Read File | FlowiseAI\\n  \\n\\nIntegrationsLangChainToolsRead FileRead file from disk.PreviousCode Interpreter by E2BNextRequest GetLast updated6 days ago',\n  'url': 'https://integrations_langchain_tools_read-file?fallback/true.html'},\n {'chunk': '\\n   Introduction | FlowiseAI\\n  \\n\\nIntroductionWelcome to the official Flowise documentationNextGet StartedLast updated6 days ago',\n  'url': 'https://#retriever-nodes.html'}]"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Convert ids to list of strings of ids for Chromadb \nids = [str(i) for i in df['chunk_id'].to_list()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:04:59.995661Z","iopub.execute_input":"2025-01-30T20:04:59.995945Z","iopub.status.idle":"2025-01-30T20:05:00.000225Z","shell.execute_reply.started":"2025-01-30T20:04:59.995924Z","shell.execute_reply":"2025-01-30T20:04:59.999179Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Initiate persistent Chroma DB client to work around the max_batch_size limit\nclient = chromadb.PersistentClient()\ncollection = client.get_or_create_collection(\"rag-chroma\")\n\nvector_store = Chroma(\n    client=client,\n    collection_name=\"rag-chroma\",\n    embedding_function=embedding_model,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:27:11.243549Z","iopub.execute_input":"2025-01-30T20:27:11.243949Z","iopub.status.idle":"2025-01-30T20:27:11.257855Z","shell.execute_reply.started":"2025-01-30T20:27:11.243918Z","shell.execute_reply":"2025-01-30T20:27:11.257173Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Use the same loop for adding documents from the list of documents\n\nstart_index = 0\nmax_batch_size = 5461\ntotal_len = len(list_of_documents)\n\nfor i in range(1, total_len//5461 + 2):\n    \n    end_index = i*5461\n\n    if 54500 - start_index < 5461:\n        vector_store.add_documents(documents=list_of_documents[start_index:], ids=ids)\n        break\n        \n    else:\n        vector_store.add_documents(documents=list_of_documents[start_index:end_index], ids=ids)\n        \n    start_index = end_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:27:13.106362Z","iopub.execute_input":"2025-01-30T20:27:13.106717Z","iopub.status.idle":"2025-01-30T20:29:05.442905Z","shell.execute_reply.started":"2025-01-30T20:27:13.106691Z","shell.execute_reply":"2025-01-30T20:29:05.441982Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/46 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88a003988d624ab3bae8d379d47b714e"}},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"retriever = vector_store.as_retriever(search_kwargs={\"k\": 7})\nretriever.invoke(\"FlowiseAI supports integration with popular large language models\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:29:05.444202Z","iopub.execute_input":"2025-01-30T20:29:05.444504Z","iopub.status.idle":"2025-01-30T20:29:05.546778Z","shell.execute_reply.started":"2025-01-30T20:29:05.444471Z","shell.execute_reply":"2025-01-30T20:29:05.545908Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fa94503ac9d47f9bcf85e33ce98c7da"}},"metadata":{}},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"[Document(metadata={'source_url': 'https://integrations?fallback/true.html'}, page_content='\\n   Integrations | FlowiseAI\\n  \\n\\nIntegrationsLearn about all available integrations / nodes in FlowiseIn Flowise, nodes are referred to as integrations. Similar to LEGO, you can build a customized LLM ochestration flow, a chatbot, an agent with all the integrations available in Flowise.LangChainAgentsCacheChainsChat ModelsDocument LoadersEmbeddingsLLMsMemoryModerationOutput ParsersPromptsRecord ManagersRetrieversText SplittersToolsVector StoresLlamaIndexAgentsChat ModelsEmbeddingsEngineResponse SynthesizerToolsVector StoresUtilitiesCustom JS FunctionSet/Get VariableIf ElseSet VariableSticky NoteExternal IntegrationsZapier ZapsPreviousRunning in ProductionNextLangChainLast updated6 days ago'),\n Document(metadata={'source_url': 'https://integrations_langchain_chat-models?fallback/true.html'}, page_content='\\n   Chat Models | FlowiseAI\\n  \\n\\nIntegrationsLangChainChat ModelsLangChain Chat Model NodesChat models take a list of messages as input and return a model-generated message as output. These models such asgpt-3.5-turboorgpt4are powerful and cheaper than its predecessor Completions models such astext-davincii-003.Chat Model Nodes:AWS ChatBedrockAzure ChatOpenAINIBittensorChatChatAnthropicChatCohereChat FireworksChatGoogleGenerativeAIChatGooglePaLMGoogle VertexAIChatHuggingFaceChatLocalAIChatMistralAIChatOllamaChatOllama FuntionChatOpenAIChatOpenAI CustomChatTogetherAIGroqChatPreviousVectorDB QA ChainNextAWS ChatBedrockLast updated6 days ago'),\n Document(metadata={'source_url': 'https://integrations_langchain_chat-models_azure-chatopenai?fallback/true.html'}, page_content='\\n   ChatOpenAI | FlowiseAI\\n  \\n\\nIntegrationsLangChainChat ModelsChatOpenAIPreviousChatOllamaNextChatTogetherAILast updated6 days ago'),\n Document(metadata={'source_url': 'https://integrations_langchain_chains_multi-prompt-chain.html'}, page_content='\\n   Multi Prompt Chain | FlowiseAI\\n  \\n\\nIntegrationsLangChainChainsMulti Prompt ChainChain automatically picks an appropriate prompt from multiple prompt templates.PreviousLLM ChainNextMulti Retrieval QA ChainLast updated6 days ago'),\n Document(metadata={'source_url': 'https://integrations_langchain_llms_azure-openai?fallback/true.html'}, page_content='\\n   Azure OpenAI | FlowiseAI\\n  \\n\\nIntegrationsLangChainLLMsAzure OpenAIWrapper around Azure OpenAI large language models.Azure OpenAI NodeThis section is a work in progress. We appreciate any help you can provide in completing this section. Please check ourContribution Guideto get started.PreviousAWS BedrockNextCohereLast updated6 days ago'),\n Document(metadata={'source_url': 'https://integrations_langchain_embeddings_openai-embeddings-custom?fallback/true.html'}, page_content='\\n   OpenAI Embeddings Custom | FlowiseAI\\n  \\n\\nIntegrationsLangChainEmbeddingsOpenAI Embeddings CustomOpenAI API to generate embeddings for a given text.PreviousOpenAI EmbeddingsNextTogetherAI EmbeddingLast updated6 days ago'),\n Document(metadata={'source_url': 'https://integrations_langchain_vector-stores_redis?fallback/true.html'}, page_content='\\n   Redis | FlowiseAI\\n  \\n\\nIntegrationsLangChainVector StoresRedisPreviousQdrantNextSingleStoreLast updated6 days ago')]"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"retriever=vector_store.as_retriever()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:29:05.548468Z","iopub.execute_input":"2025-01-30T20:29:05.548686Z","iopub.status.idle":"2025-01-30T20:29:05.552079Z","shell.execute_reply.started":"2025-01-30T20:29:05.548667Z","shell.execute_reply":"2025-01-30T20:29:05.551257Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"# Retrieval grader","metadata":{}},{"cell_type":"code","source":"# Prompt\nprompt = PromptTemplate.from_template(\n\"\"\"\nYou are an assistant for responding to Request For Proposal documents for a \nbidder in the field of Data Science and Engineering. \nYou will be given technical requirements point. \nUse the following pieces of retrieved context to respond to the requests. \nIf you don't know the answer, just say that you don't know. \nAnalyze the request carefully and make sure that your answer \naddresses the topic of the request. Base your answer on the most \nrelevant piece of context.\n\nQuestion: {question} \nContext: {context} \nAnswer:\n\"\"\"\n)\n\n\n# LLM\nllm = ChatNVIDIA(model=\"meta/llama-3.3-70b-instruct\", temperature=0)\n\n# Post-processing\ndef format_docs(result):\n    return \"\\n\\n\".join(doc.page_content for doc in result)\n\n# Chain\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\n# Run\n# generation = rag_chain.invoke(question)\n# print(generation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:29:05.553373Z","iopub.execute_input":"2025-01-30T20:29:05.553692Z","iopub.status.idle":"2025-01-30T20:29:05.565791Z","shell.execute_reply.started":"2025-01-30T20:29:05.553667Z","shell.execute_reply":"2025-01-30T20:29:05.564864Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"### Retrieval grader\n\n# Data model\nclass GradeDocuments(BaseModel):\n    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n\n    binary_score: str = Field(\n        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n    )\n\n\n# LLM\nllm = ChatNVIDIA(model=\"meta/llama-3.3-70b-instruct\", temperature=0)\n\n# Structured output\nstructured_llm_grader = llm.with_structured_output(GradeDocuments)\n\n# Prompt\nsystem = \"\"\"You are a grader assessing relevance of a retrieved document to a client request. \\n \n    If the document contains keyword(s) or semantic meaning related to the client request, grade it as relevant. \\n\n    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n    Analyze the request and the documents and decide whether FlowiseAI complies with the request based on the retrieved documents. \\n\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\ngrade_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n    ]\n)\n\nretrieval_grader = grade_prompt | structured_llm_grader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:29:05.566832Z","iopub.execute_input":"2025-01-30T20:29:05.567146Z","iopub.status.idle":"2025-01-30T20:29:05.684226Z","shell.execute_reply.started":"2025-01-30T20:29:05.567114Z","shell.execute_reply":"2025-01-30T20:29:05.683611Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"# Web Search Tool","metadata":{}},{"cell_type":"code","source":"web_search_tool = TavilySearchResults(k=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:29:05.685000Z","iopub.execute_input":"2025-01-30T20:29:05.685271Z","iopub.status.idle":"2025-01-30T20:29:05.688864Z","shell.execute_reply.started":"2025-01-30T20:29:05.685242Z","shell.execute_reply":"2025-01-30T20:29:05.688206Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# Web search\ndocs = web_search_tool.invoke({\"query\": \"FlowiseAI on premise installation\"})\nweb_results = \"\\n\".join([d[\"content\"] for d in docs])\nweb_results = Document(page_content=web_results)\nprint(web_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:29:05.689556Z","iopub.execute_input":"2025-01-30T20:29:05.689743Z","iopub.status.idle":"2025-01-30T20:29:08.329265Z","shell.execute_reply.started":"2025-01-30T20:29:05.689726Z","shell.execute_reply":"2025-01-30T20:29:08.328562Z"}},"outputs":[{"name":"stdout","text":"page_content='Follow our installation guide to start using FlowiseAI. 1. Login or create an account. To access Elestio, you will need to connect to your existing account or create a new one. We provide 4 ways to connect to your account: Github; Gitlab; Google; Email; MFA is enable by default for new accounts created with an email address.\nFlowise AI Modules. Flowise AI breaks down into three core modules: Server: Handles data processing, operations, task execution UI: The graphical interface for interacting with Flowise AI Components: Individual building blocks that provide functionality. Together these modules create the Flowise AI ecosystem. But each can also be tweaked or\nIt is free to deploy the app on Huggingface. Flowise is not available as a one-click app on Coolify, but you can install it with Dockerfile. DATABASE_PATH=/app/storage APIKEY_PATH=/app/storage LOG_PATH=/app/storage SECRETKEY_PATH=/app/storage Step3: Set the Persistent Storage (destination path) as /app/storage FlowiseAI uses SQlite as its database by default, but Coolify currently does not have backup method for SQLite. Step6: Use cyberduck or filezilla to sync the bind mount database (.sqlite file) to your local folder. Step7: If you want to backup to S3 compatible storage, use Cyberduck to connect to your bucket. Set Persistent Storage (destination path aka volume mount) as /app/storage. APIKEY_PATH=/app/storage LOG_PATH=/app/storage SECRETKEY_PATH=/app/storage DATABASE_NAME= You can now back up your PostgreSQL database to your S3 storage.\nThis video provides a comprehensive tutorial on setting up and using Flowise, an open-source tool for building AI chatbots. It covers the process of deploying Flowise on your own hosting using Render, setting up necessary credentials, and creating a functional chatbot with a knowledge base and memory. The tutorial explains how to integrate various components like OpenAI embeddings, Qdrant\nModern cloud platforms prioritize automation and focus on developer workflows, simplifying cloud management and ongoing maintenance. This reduces the technical expertise needed, but may limit the level of customization you have over the underlying infrastructure.'\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"# Langgraph","metadata":{}},{"cell_type":"code","source":"class GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        question: question\n        generation: LLM generation\n        documents: list of documents\n    \"\"\"\n\n    question: str\n    generation: str\n    decision: str\n    documents: List[str]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:29:08.330932Z","iopub.execute_input":"2025-01-30T20:29:08.331192Z","iopub.status.idle":"2025-01-30T20:29:08.335640Z","shell.execute_reply.started":"2025-01-30T20:29:08.331158Z","shell.execute_reply":"2025-01-30T20:29:08.334709Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# Nodes langgraph (python functions)\n\ndef retrieve(state):\n    \"\"\"\n    Retrieve documents\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, documents, that contains retrieved documents\n    \"\"\"\n    print(\"---RETRIEVE---\")\n    question = state[\"question\"]\n\n    # Retrieval\n    documents = retriever.invoke(question)\n    \n    return {\"documents\": documents, \"question\": question}\n\n\ndef retrieval_grader_function(state):\n\n\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # Score each doc\n    filtered_docs = []\n    for d in documents:\n        score = retrieval_grader.invoke(\n            {\"question\": question, \"document\": d.page_content}\n        )\n        grade = score.binary_score\n        if grade == \"yes\":\n            # print(\"---GRADE: DOCUMENT RELEVANT---\")\n            filtered_docs.append(d)\n        else:\n            # print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n            continue\n\n\n    if len(filtered_docs) == 0:\n\n        print(\"---NO RELEVANT DOCUMENTS FOUND---\")\n\n        print(\"\\nNo relevant documents were found. going to websearch ...\")\n\n        print(\"\\nDecision is Not Compliant ...\")\n\n        return {\"question\": question, \"decision\": \"Partially Compliant or Not Compliant\"}\n    \n    else:\n        print(\"---RELEVANT DOCUMENTS FILTERED---\")\n        print(\"decision:\", len(filtered_docs), \"Relevant documents found and will be used to answer\")\n\n        return {\"documents\": filtered_docs, \"question\": question, \"decision\": \"Fully Compliant\"}\n\n    \n\ndef compliance(state):\n\n    print(\"---CHECK SOURCE---\")\n    question = state[\"question\"]\n    # documents = state[\"documents\"]\n    # generation = state[\"generation\"]\n\n    \n    comp = question_router.invoke({\"client_request\": question})\n\n    compliance_decision = comp.ComplianceType\n\n\n\n    return {\"decision\": compliance_decision, \"question\": question}\n\n\ndef condition(state):\n\n    dec = state[\"decision\"]\n\n    print(dec)\n\n    if dec == \"Fully Compliant\":\n        print(\"Decision: The request is fully compliant and local document will be used: \\n \\n\")\n\n        return \"FC\"\n    \n    # elif dec == \"Partially Compliant or Not Compliant\":\n    #     print(\"Decision: The request is partially compliant or not compliant and web search will be used.\")\n\n    #     return \"PC_or_NC\"\n    \n    else:\n        print(\"Decision: The request is partially compliant or not compliant.\")\n\n\n        return \"PC or NC\"\n\n\n\n\ndef generate_rag(state):\n\n    print(\"I should be running only when the decision is Full Compliant\")\n\n    question = state[\"question\"]\n    decision = state[\"decision\"]\n    \n    generation = rag_chain.invoke(question)\n\n    return {\"generation\": generation}\n\n\ndef web_search(state):\n    \"\"\"\n    Web search based on the re-phrased question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Updates documents key with appended web results\n    \"\"\"\n\n    print(\"---WEB SEARCH---\")\n    question = state[\"question\"]\n\n    # Web search\n    docs = web_search_tool.invoke({\"query\": question})\n    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n    web_results = Document(page_content=web_results)\n\n    return {\"documents\": web_results, \"question\": question}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:29:08.336506Z","iopub.execute_input":"2025-01-30T20:29:08.336735Z","iopub.status.idle":"2025-01-30T20:29:08.353173Z","shell.execute_reply.started":"2025-01-30T20:29:08.336703Z","shell.execute_reply":"2025-01-30T20:29:08.352251Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"workflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"Retrieve\", retrieve)\nworkflow.add_node(\"Retrieval grader\", retrieval_grader_function)\nworkflow.add_node(\"generate_rag\", generate_rag)  # use RAG\nworkflow.add_node(\"web_search\", web_search)\n\n# Build graph\nworkflow.add_edge(START, \"Retrieve\")\nworkflow.add_edge(\"Retrieve\", \"Retrieval grader\")\nworkflow.add_conditional_edges(\n    \"Retrieval grader\",\n    condition,\n    {\n        \"FC\": \"generate_rag\",\n        \"PC or NC\": \"web_search\"\n    },\n    \n)\n\nworkflow.add_edge(\"web_search\", \"generate_rag\")\nworkflow.add_edge(\"generate_rag\", END)\n\n\n\n# Compile\napp = workflow.compile()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:29:08.354145Z","iopub.execute_input":"2025-01-30T20:29:08.354425Z","iopub.status.idle":"2025-01-30T20:29:08.364663Z","shell.execute_reply.started":"2025-01-30T20:29:08.354388Z","shell.execute_reply":"2025-01-30T20:29:08.363891Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"# Load the requirements file.","metadata":{}},{"cell_type":"code","source":"cm=pd.read_csv(\"/kaggle/input/requirement/requirements.csv\",encoding='latin-1')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:29:08.365436Z","iopub.execute_input":"2025-01-30T20:29:08.365747Z","iopub.status.idle":"2025-01-30T20:29:08.394956Z","shell.execute_reply.started":"2025-01-30T20:29:08.365715Z","shell.execute_reply":"2025-01-30T20:29:08.394353Z"}},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":"# Run the Graph","metadata":{}},{"cell_type":"code","source":"for request in cm:\n\n    # Run\n    inputs = {\n        \"question\": request\n    }\n\n    for output in app.stream(inputs):\n        for key, value in output.items():\n            # Node\n            pprint(f\"Node '{key}':\")\n            # Optional: print full state at each node\n            # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n        pprint(\"\\n---\\n\")\n\n    \n    # Final generation\n    pprint(value[\"generation\"])\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T20:29:08.395568Z","iopub.execute_input":"2025-01-30T20:29:08.395762Z","iopub.status.idle":"2025-01-30T20:29:35.852160Z","shell.execute_reply.started":"2025-01-30T20:29:08.395744Z","shell.execute_reply":"2025-01-30T20:29:35.851404Z"}},"outputs":[{"name":"stdout","text":"---RETRIEVE---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b2a5b188597491f84b04e7b73bbcab8"}},"metadata":{}},{"name":"stdout","text":"\"Node 'Retrieve':\"\n'\\n---\\n'\n---CHECK DOCUMENT RELEVANCE TO QUESTION---\n---RELEVANT DOCUMENTS FILTERED---\ndecision: 2 Relevant documents found and will be used to answer\nFully Compliant\nDecision: The request is fully compliant and local document will be used: \n \n\n\"Node 'Retrieval grader':\"\n'\\n---\\n'\nI should be running only when the decision is Full Compliant\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e337498cdd0419386f75e21a5b4fb45"}},"metadata":{}},{"name":"stdout","text":"\"Node 'generate_rag':\"\n'\\n---\\n'\n('The solution provides capabilities to connect to NoSQL databases, '\n 'specifically MongoDB Atlas, a managed cloud MongoDB database. This is '\n 'evident from the context, which mentions \"Upsert embedded data and perform '\n 'similarity or mmr search upon query using MongoDB Atlas\". This indicates '\n 'that the solution has the ability to connect to MongoDB Atlas, a NoSQL '\n 'database, and perform various operations on it. \\n'\n '\\n'\n \"However, it's worth noting that the context also mentions support for other \"\n 'database types, including SQLite, MySQL, PostgreSQL, and MariaDB, but these '\n 'are primarily relational databases, not NoSQL databases. Therefore, the '\n \"solution's capability to connect to NoSQL databases is specifically \"\n 'highlighted through its support for MongoDB Atlas.')\n---RETRIEVE---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ef58427b642423fa41d5465d556705d"}},"metadata":{}},{"name":"stdout","text":"\"Node 'Retrieve':\"\n'\\n---\\n'\n---CHECK DOCUMENT RELEVANCE TO QUESTION---\n---RELEVANT DOCUMENTS FILTERED---\ndecision: 4 Relevant documents found and will be used to answer\nFully Compliant\nDecision: The request is fully compliant and local document will be used: \n \n\n\"Node 'Retrieval grader':\"\n'\\n---\\n'\nI should be running only when the decision is Full Compliant\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9850479f13743b9873a5b4212be41b9"}},"metadata":{}},{"name":"stdout","text":"\"Node 'generate_rag':\"\n'\\n---\\n'\n('Based on the provided context, it appears that the bidder has experience '\n 'integrating with MongoDB, specifically using MongoDB Atlas, a managed cloud '\n 'MongoDB database. They have used it for various purposes, such as:\\n'\n '\\n'\n '1. Upserting embedded data and performing similarity or MMR search upon '\n 'query.\\n'\n '2. Storing conversation data in MongoDB Atlas for chat memory purposes.\\n'\n '\\n'\n 'Therefore, the answer to the request is that the bidder has experience with '\n 'MongoDB, particularly with MongoDB Atlas, and can leverage it for data '\n 'storage and querying purposes.')\n---RETRIEVE---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25d452a27c03426da4d71774200990a7"}},"metadata":{}},{"name":"stdout","text":"\"Node 'Retrieve':\"\n'\\n---\\n'\n---CHECK DOCUMENT RELEVANCE TO QUESTION---\n---RELEVANT DOCUMENTS FILTERED---\ndecision: 1 Relevant documents found and will be used to answer\nFully Compliant\nDecision: The request is fully compliant and local document will be used: \n \n\n\"Node 'Retrieval grader':\"\n'\\n---\\n'\nI should be running only when the decision is Full Compliant\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1ec90eb3f0340d192260476ae6271bc"}},"metadata":{}},{"name":"stdout","text":"\"Node 'generate_rag':\"\n'\\n---\\n'\n('Based on the provided context, it appears that ElasticSearch is mentioned in '\n 'the first section: \"Elastic | FlowiseAI\" under the category of '\n '\"IntegrationsLangChainVector Stores\". This suggests that FlowiseAI has an '\n 'integration with ElasticSearch, which is a search and analytics engine. '\n 'However, the context does not provide detailed information about the '\n 'specific capabilities or implementation of ElasticSearch within FlowiseAI. \\n'\n '\\n'\n 'Therefore, the answer to the question about ElasticSearch is that FlowiseAI '\n 'has an integration with it, but the details of this integration are not '\n 'specified in the provided context.')\n---RETRIEVE---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b4266ec4a244658805b8c054c06398a"}},"metadata":{}},{"name":"stdout","text":"\"Node 'Retrieve':\"\n'\\n---\\n'\n---CHECK DOCUMENT RELEVANCE TO QUESTION---\n---NO RELEVANT DOCUMENTS FOUND---\n\nNo relevant documents were found. going to websearch ...\n\nDecision is Not Compliant ...\nPartially Compliant or Not Compliant\nDecision: The request is partially compliant or not compliant.\n\"Node 'Retrieval grader':\"\n'\\n---\\n'\n---WEB SEARCH---\n\"Node 'web_search':\"\n'\\n---\\n'\nI should be running only when the decision is Full Compliant\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"047a5f6f91ca405bb8cdc9a32eaafbad"}},"metadata":{}},{"name":"stdout","text":"\"Node 'generate_rag':\"\n'\\n---\\n'\n(\"I don't know about CouchDB as it is not mentioned in the provided context. \"\n 'The context only mentions other databases and technologies such as '\n 'SingleStore, Elastic, Redis, Supabase, and SQL Database Chain, but does not '\n 'provide any information about CouchDB.')\n---RETRIEVE---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd2fe66c7a594c0d85dcbe03c83314eb"}},"metadata":{}},{"name":"stdout","text":"\"Node 'Retrieve':\"\n'\\n---\\n'\n---CHECK DOCUMENT RELEVANCE TO QUESTION---\n---RELEVANT DOCUMENTS FILTERED---\ndecision: 4 Relevant documents found and will be used to answer\nFully Compliant\nDecision: The request is fully compliant and local document will be used: \n \n\n\"Node 'Retrieval grader':\"\n'\\n---\\n'\nI should be running only when the decision is Full Compliant\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f4684cca71445f48f026831add4213b"}},"metadata":{}},{"name":"stdout","text":"\"Node 'generate_rag':\"\n'\\n---\\n'\n('It seems like there is no actual question provided, just a series of context '\n 'snippets related to FlowiseAI and Elastic. Could you please provide the '\n \"actual question or technical requirement you'd like me to respond to? I'll \"\n 'do my best to address it based on the given context or let you know if I '\n \"don't have enough information to provide an answer.\")\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}